#!/usr/bin/env python3
"""
AI Fusion Studio ãƒ¢ãƒ€ãƒ³Webãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰
Streamlitãƒ™ãƒ¼ã‚¹ã®ãƒ—ãƒ­ãƒ•ã‚§ãƒƒã‚·ãƒ§ãƒŠãƒ«ãªãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹
"""

import streamlit as st
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import json
import yaml
import os
import sys
from pathlib import Path
from datetime import datetime
import subprocess

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
sys.path.append(str(Path(__file__).parent.parent))

# Streamlitè¨­å®š
st.set_page_config(
    page_title="AI Fusion Studio ğŸš€",
    page_icon="ğŸš€",
    layout="wide",
    initial_sidebar_state="expanded"
)

# ã‚«ã‚¹ã‚¿ãƒ CSS - é«˜ã‚³ãƒ³ãƒˆãƒ©ã‚¹ãƒˆãƒ»å¯èª­æ€§é‡è¦–ãƒ‡ã‚¶ã‚¤ãƒ³
st.markdown("""
<style>
    @import url('https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap');
    
    .main {
        font-family: 'Inter', sans-serif;
        background-color: #f8fafc;
    }
    
    .main-header {
        background: linear-gradient(135deg, #1e3a8a 0%, #3730a3 100%);
        padding: 3rem 2rem;
        border-radius: 16px;
        color: white;
        text-align: center;
        margin-bottom: 2rem;
        box-shadow: 0 8px 32px rgba(30, 58, 138, 0.3);
    }
    
    .main-header h1 {
        font-size: 2.5rem !important;
        font-weight: 700 !important;
        margin-bottom: 1rem !important;
        color: white !important;
        text-shadow: 2px 2px 4px rgba(0,0,0,0.3);
    }
    
    .main-header p {
        font-size: 1.1rem !important;
        font-weight: 400 !important;
        color: rgba(255,255,255,0.95) !important;
        text-shadow: 1px 1px 2px rgba(0,0,0,0.2);
        margin-bottom: 0 !important;
    }
    
    .metric-card {
        background: white;
        padding: 2rem 1.5rem;
        border-radius: 12px;
        border: 1px solid #e2e8f0;
        box-shadow: 0 4px 16px rgba(0,0,0,0.08);
        margin: 1rem 0;
        transition: all 0.2s ease;
        border-left: 4px solid #3b82f6;
    }
    
    .metric-card:hover {
        transform: translateY(-2px);
        box-shadow: 0 8px 24px rgba(0,0,0,0.12);
    }
    
    .metric-value {
        font-size: 2.5rem;
        font-weight: 700;
        color: #1e293b;
        line-height: 1;
        margin-bottom: 0.5rem;
    }
    
    .metric-label {
        font-size: 0.95rem;
        color: #475569;
        font-weight: 600;
        text-transform: uppercase;
        letter-spacing: 0.5px;
    }
    
    .experiment-card {
        background: white;
        padding: 1.5rem;
        border-radius: 12px;
        box-shadow: 0 2px 8px rgba(0,0,0,0.1);
        border: 1px solid #e2e8f0;
        margin: 1rem 0;
        transition: all 0.2s ease;
    }
    
    .experiment-card:hover {
        transform: translateY(-1px);
        box-shadow: 0 4px 16px rgba(0,0,0,0.15);
    }
    
    .status-badge {
        padding: 0.5rem 1rem;
        border-radius: 16px;
        font-size: 0.8rem;
        font-weight: 600;
        text-transform: uppercase;
        letter-spacing: 0.5px;
        display: inline-block;
    }
    
    .status-completed {
        background: #059669;
        color: white;
    }
    
    .status-running {
        background: #d97706;
        color: white;
    }
    
    .status-failed {
        background: #dc2626;
        color: white;
    }
    
    .status-unknown {
        background: #6b7280;
        color: white;
    }
    
    .dashboard-section {
        background: white;
        border-radius: 16px;
        padding: 2rem;
        margin: 1.5rem 0;
        box-shadow: 0 2px 8px rgba(0,0,0,0.06);
        border: 1px solid #f1f5f9;
    }
    
    .section-title {
        font-size: 1.5rem;
        font-weight: 600;
        color: #1e293b;
        margin-bottom: 1.5rem;
        display: flex;
        align-items: center;
        gap: 0.5rem;
    }
    
    .recent-experiment {
        background: #f8fafc;
        padding: 1.5rem;
        border-radius: 12px;
        margin: 1rem 0;
        border: 1px solid #e2e8f0;
        border-left: 4px solid #3b82f6;
        transition: all 0.2s ease;
    }
    
    .recent-experiment:hover {
        background: #f1f5f9;
        transform: translateX(4px);
    }
    
    .score-highlight {
        background: linear-gradient(135deg, #3b82f6, #1d4ed8);
        color: white;
        padding: 0.5rem 1rem;
        border-radius: 20px;
        font-weight: 600;
        font-size: 0.9rem;
        display: inline-block;
        box-shadow: 0 2px 8px rgba(59, 130, 246, 0.3);
    }
    
    /* Streamlit ì»´í¬ë„ŒíŠ¸ ìŠ¤íƒ€ì¼ë§ */
    .stSelectbox > div > div {
        background: white;
        border: 2px solid #e2e8f0;
        border-radius: 8px;
        color: #1e293b;
    }
    
    .stButton > button {
        background: linear-gradient(135deg, #3b82f6 0%, #1d4ed8 100%);
        color: white;
        border: none;
        border-radius: 8px;
        padding: 0.75rem 1.5rem;
        font-weight: 600;
        transition: all 0.2s ease;
    }
    
    .stButton > button:hover {
        transform: translateY(-1px);
        box-shadow: 0 4px 16px rgba(59, 130, 246, 0.3);
    }
    
    .stMetric {
        background: white;
        padding: 1rem;
        border-radius: 8px;
        border: 1px solid #e2e8f0;
    }
    
    .stMetric [data-testid="metric-container"] {
        background: white;
        border: 1px solid #e2e8f0;
        padding: 1rem;
        border-radius: 8px;
    }
    
    /* í…ìŠ¤íŠ¸ ê°€ë…ì„± ê°•í™” */
    h1, h2, h3, h4, h5, h6 {
        color: #1e293b !important;
    }
    
    p, div, span {
        color: #334155 !important;
    }
    
    .stMarkdown {
        color: #334155 !important;
    }
</style>
""", unsafe_allow_html=True)


class AIFusionStudioApp:
    """ãƒ¡ã‚¤ãƒ³ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.experiments_dir = Path("experiments")
        self.configs_dir = Path("configs")
        self.models_dir = Path("models")
        
        # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
        for dir_path in [self.experiments_dir, self.configs_dir, self.models_dir]:
            dir_path.mkdir(exist_ok=True)
    
    def load_experiments(self):
        """å®Ÿé¨“ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿"""
        db_path = self.experiments_dir / "experiments_db.json"
        if db_path.exists():
            with open(db_path, 'r') as f:
                return json.load(f)
        return []
    
    def load_configs(self):
        """åˆ©ç”¨å¯èƒ½ãªè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—"""
        configs = []
        for config_file in self.configs_dir.glob("*.yaml"):
            try:
                with open(config_file, 'r') as f:
                    config = yaml.safe_load(f)
                    config['filename'] = config_file.name
                    configs.append(config)
            except Exception as e:
                st.error(f"è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {config_file.name} - {e}")
        return configs
    
    def run_experiment(self, config_file, skip_steps=None):
        """å®Ÿé¨“ã‚’å®Ÿè¡Œ"""
        cmd = ["python", "scripts/run_experiment.py", str(config_file)]
        if skip_steps:
            cmd.extend(["--skip"] + skip_steps)
        
        try:
            result = subprocess.run(cmd, capture_output=True, text=True, cwd=".")
            return result.returncode == 0, result.stdout, result.stderr
        except Exception as e:
            return False, "", str(e)


def main():
    app = AIFusionStudioApp()
    
    # ãƒ˜ãƒƒãƒ€ãƒ¼
    st.markdown("""
    <div class="main-header">
        <h1>ğŸš€ AI Fusion Studio</h1>
        <p>æœ€å¼·ã®AIãƒ¢ãƒ‡ãƒ«ã‚’èåˆã•ã›ã‚‹ãƒ—ãƒ­ãƒ•ã‚§ãƒƒã‚·ãƒ§ãƒŠãƒ«ã‚¹ã‚¿ã‚¸ã‚ª</p>
    </div>
    """, unsafe_allow_html=True)
    
    # ã‚µã‚¤ãƒ‰ãƒãƒ¼
    with st.sidebar:
        st.image("https://via.placeholder.com/200x100/667eea/white?text=AI+Fusion", width=200)
        
        page = st.selectbox(
            "ğŸ”§ æ©Ÿèƒ½ã‚’é¸æŠ",
            ["ğŸ“Š ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰", "ğŸš€ æ–°ã—ã„å®Ÿé¨“", "ğŸ“ˆ å®Ÿé¨“çµæœ", "âš™ï¸ è¨­å®šç®¡ç†", "ğŸ“š ã‚¬ã‚¤ãƒ‰", "ğŸ” å®Ÿé¨“æ¯”è¼ƒ", "ğŸ“ ãƒ‡ãƒ¼ã‚¿ç®¡ç†"]
        )
        
        st.markdown("---")
        
        # ã‚¯ã‚¤ãƒƒã‚¯ã‚¢ã‚¯ã‚·ãƒ§ãƒ³
        st.markdown("### âš¡ ã‚¯ã‚¤ãƒƒã‚¯ã‚¢ã‚¯ã‚·ãƒ§ãƒ³")
        
        if st.button("ğŸ¯ æ¨å¥¨å®Ÿé¨“å®Ÿè¡Œ", use_container_width=True):
            with st.spinner("Gemma Ã— Qwenå®Ÿé¨“ã‚’å®Ÿè¡Œä¸­..."):
                success, stdout, stderr = app.run_experiment("configs/gemma_qwen_slerp.yaml")
                if success:
                    st.success("å®Ÿé¨“ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
                else:
                    st.error(f"å®Ÿé¨“ãŒå¤±æ•—ã—ã¾ã—ãŸ: {stderr}")
        
        if st.button("ğŸ“Š ãƒªãƒ¼ãƒ€ãƒ¼ãƒœãƒ¼ãƒ‰æ›´æ–°", use_container_width=True):
            st.rerun()
    
    # ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„
    if page == "ğŸ“Š ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰":
        show_dashboard(app)
    elif page == "ğŸš€ æ–°ã—ã„å®Ÿé¨“":
        show_new_experiment(app)
    elif page == "ğŸ“ˆ å®Ÿé¨“çµæœ":
        show_experiment_results(app)
    elif page == "âš™ï¸ è¨­å®šç®¡ç†":
        show_config_management(app)
    elif page == "ğŸ“š ã‚¬ã‚¤ãƒ‰":
        show_guide()
    elif page == "ğŸ” å®Ÿé¨“æ¯”è¼ƒ":
        show_experiment_comparison(app.load_experiments())
    elif page == "ğŸ“ ãƒ‡ãƒ¼ã‚¿ç®¡ç†":
        show_data_management(app)


def show_dashboard(app):
    """ãƒ¢ãƒ€ãƒ³ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰è¡¨ç¤º"""
    # ãƒ¡ã‚¤ãƒ³ãƒ˜ãƒƒãƒ€ãƒ¼ - é«˜ã‚³ãƒ³ãƒˆãƒ©ã‚¹ãƒˆãƒ»å¯èª­æ€§é‡è¦–
    st.markdown("""
    <div class="main-header">
        <h1>ğŸš€ AI Fusion Studio</h1>
        <p>æœ€å¼·ã®AIãƒ¢ãƒ‡ãƒ«ã‚’èåˆã•ã›ã‚‹ãƒ—ãƒ­ãƒ•ã‚§ãƒƒã‚·ãƒ§ãƒŠãƒ«ã‚¹ã‚¿ã‚¸ã‚ª</p>
        <div style="margin-top: 1.5rem;">
            <span style="background: rgba(255,255,255,0.25); padding: 0.5rem 1rem; border-radius: 20px; margin: 0 0.5rem; 
                         color: white; font-weight: 500; font-size: 0.9rem;">ğŸ”¬ Advanced AI Fusion</span>
            <span style="background: rgba(255,255,255,0.25); padding: 0.5rem 1rem; border-radius: 20px; margin: 0 0.5rem; 
                         color: white; font-weight: 500; font-size: 0.9rem;">ğŸ¢ Enterprise Ready</span>
            <span style="background: rgba(255,255,255,0.25); padding: 0.5rem 1rem; border-radius: 20px; margin: 0 0.5rem; 
                         color: white; font-weight: 500; font-size: 0.9rem;">â­ Production Grade</span>
        </div>
    </div>
    """, unsafe_allow_html=True)
    
    # å®Ÿé¨“ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
    experiments = app.load_experiments()
    
    if not experiments:
        st.markdown("""
        <div class="dashboard-section">
            <div class="section-title">ğŸ¯ å®Ÿé¨“ã‚’é–‹å§‹ã—ã¾ã—ã‚‡ã†</div>
            <p style="font-size: 1.2rem; color: #64748b; margin-bottom: 2rem;">
            ã¾ã å®Ÿé¨“ãŒå®Ÿè¡Œã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚ã‚µã‚¤ãƒ‰ãƒãƒ¼ã‹ã‚‰ã€ŒğŸš€ æ–°ã—ã„å®Ÿé¨“ã€ã‚’é¸æŠã—ã¦ã€
            æœ€åˆã®ãƒ¢ãƒ‡ãƒ«ãƒãƒ¼ã‚¸å®Ÿé¨“ã‚’é–‹å§‹ã—ã¦ãã ã•ã„ã€‚
            </p>
            <div style="background: linear-gradient(135deg, #f0f9ff 0%, #e0f2fe 100%); 
                        padding: 1.5rem; border-radius: 12px; border-left: 4px solid #0ea5e9;">
                <strong>ğŸ’¡ æ¨å¥¨:</strong> Gemma Ã— Qwen SLERPå®Ÿé¨“ã‹ã‚‰å§‹ã‚ã‚‹ã“ã¨ã‚’ãŠå‹§ã‚ã—ã¾ã™
            </div>
        </div>
        """, unsafe_allow_html=True)
        return
    
    # çµ±è¨ˆæƒ…å ± - ãƒ¢ãƒ€ãƒ³ãªã‚«ãƒ¼ãƒ‰ãƒ‡ã‚¶ã‚¤ãƒ³
    st.markdown('<div class="section-title">ğŸ“Š å®Ÿé¨“çµ±è¨ˆ</div>', unsafe_allow_html=True)
    col1, col2, col3, col4 = st.columns(4)
    
    total_experiments = len(experiments)
    completed_experiments = len([e for e in experiments if e.get('status') == 'completed'])
    
    with col1:
        st.markdown("""
        <div class="metric-card">
            <div class="metric-value">{}</div>
            <div class="metric-label">ğŸ“Š ç·å®Ÿé¨“æ•°</div>
        </div>
        """.format(total_experiments), unsafe_allow_html=True)
    
    with col2:
        st.markdown("""
        <div class="metric-card">
            <div class="metric-value">{}</div>
            <div class="metric-label">âœ… å®Œäº†æ¸ˆã¿</div>
        </div>
        """.format(completed_experiments), unsafe_allow_html=True)
    
    with col3:
        if completed_experiments > 0:
            best_score = max([e.get('evaluations', {}).get('mt_bench_jp', {}).get('overall_score', 0) 
                            for e in experiments if e.get('status') == 'completed'])
            st.markdown("""
            <div class="metric-card">
                <div class="metric-value">{:.2f}</div>
                <div class="metric-label">ğŸ† æœ€é«˜ã‚¹ã‚³ã‚¢</div>
            </div>
            """.format(best_score), unsafe_allow_html=True)
        else:
            st.markdown("""
            <div class="metric-card">
                <div class="metric-value">--</div>
                <div class="metric-label">ğŸ† æœ€é«˜ã‚¹ã‚³ã‚¢</div>
            </div>
            """, unsafe_allow_html=True)
    
    with col4:
        success_rate = (completed_experiments / total_experiments * 100) if total_experiments > 0 else 0
        st.markdown("""
        <div class="metric-card">
            <div class="metric-value">{:.1f}%</div>
            <div class="metric-label">ğŸ“ˆ æˆåŠŸç‡</div>
        </div>
        """.format(success_rate), unsafe_allow_html=True)
    
    # å®Ÿé¨“çµæœã®å¯è¦–åŒ–
    if completed_experiments > 0:
        df = create_experiments_dataframe(experiments)
        
        col1, col2 = st.columns(2)
        
        with col1:
            st.subheader("ğŸ¯ MT-Benchã‚¹ã‚³ã‚¢åˆ†å¸ƒ")
            if 'mt_bench_score' in df.columns:
                fig = px.histogram(df, x='mt_bench_score', nbins=10, 
                                 title="MT-Benchã‚¹ã‚³ã‚¢åˆ†å¸ƒ",
                                 color_discrete_sequence=['#667eea'])
                fig.update_layout(showlegend=False)
                st.plotly_chart(fig, use_container_width=True)
        
        with col2:
            st.subheader("âš–ï¸ æ‰‹æ³•åˆ¥æ€§èƒ½æ¯”è¼ƒ")
            if 'method' in df.columns and 'mt_bench_score' in df.columns:
                method_avg = df.groupby('method')['mt_bench_score'].mean().reset_index()
                fig = px.bar(method_avg, x='method', y='mt_bench_score',
                           title="æ‰‹æ³•åˆ¥å¹³å‡ã‚¹ã‚³ã‚¢",
                           color='mt_bench_score',
                           color_continuous_scale='Blues')
                st.plotly_chart(fig, use_container_width=True)
        
        # æœ€è¿‘ã®å®Ÿé¨“ - ãƒ¢ãƒ€ãƒ³ãªãƒ‡ã‚¶ã‚¤ãƒ³
        st.markdown('<div class="section-title">ğŸ•’ æœ€è¿‘ã®å®Ÿé¨“</div>', unsafe_allow_html=True)
        recent_experiments = sorted(experiments, 
                                  key=lambda x: x.get('timestamp', ''), 
                                  reverse=True)[:5]
        
        st.markdown('<div class="dashboard-section">', unsafe_allow_html=True)
        for exp in recent_experiments:
            show_modern_experiment_card(exp)
        st.markdown('</div>', unsafe_allow_html=True)


def show_new_experiment(app):
    """æ–°ã—ã„å®Ÿé¨“è¨­å®šç”»é¢"""
    st.header("ğŸš€ æ–°ã—ã„å®Ÿé¨“ã‚’é–‹å§‹")
    
    # è¨­å®šé¸æŠ
    configs = app.load_configs()
    
    if not configs:
        st.error("è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚Šã¾ã›ã‚“ã€‚configs/ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’é…ç½®ã—ã¦ãã ã•ã„ã€‚")
        return
    
    # ã‚¿ãƒ–ã§è¡¨ç¤º
    tab1, tab2 = st.tabs(["ğŸ“‹ æ—¢å­˜è¨­å®šã‹ã‚‰é¸æŠ", "âœï¸ ã‚«ã‚¹ã‚¿ãƒ è¨­å®š"])
    
    with tab1:
        # æ¨å¥¨è¨­å®šã®ã‚«ãƒ¼ãƒ‰è¡¨ç¤º
        st.subheader("ğŸŒŸ æ¨å¥¨è¨­å®š")
        
        recommended_configs = [
            {
                'name': 'Gemma Ã— Qwen SLERP',
                'filename': 'gemma_qwen_slerp.yaml',
                'description': 'ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ã§8.5/10ã®ã‚¹ã‚³ã‚¢ã‚’è¨˜éŒ²ã—ãŸæœ€å¼·ã®çµ„ã¿åˆã‚ã›',
                'expected_score': 8.5,
                'use_case': 'æ±ç”¨æ—¥æœ¬èªã‚¿ã‚¹ã‚¯',
                'difficulty': 'åˆç´š',
                'time': '30åˆ†'
            },
            {
                'name': 'EvoLLM-JPå†ç¾',
                'filename': 'evolllm_jp_reproduction.yaml',
                'description': 'æ—¥æœ¬èªÃ—æ•°ç†ã®é€²åŒ–çš„ãƒãƒ¼ã‚¸ã§æ•°å­¦ã‚¿ã‚¹ã‚¯ã‚’å¼·åŒ–',
                'expected_score': 7.3,
                'use_case': 'æ•°ç†æ¨è«–',
                'difficulty': 'ä¸­ç´š',
                'time': '60åˆ†'
            },
            {
                'name': 'Gemma + Swallow LoRA',
                'filename': 'gemma_swallow_lora.yaml',
                'description': 'ç°¡å˜å®Ÿè£…ã§æ—¥æœ¬èªã®è‡ªç„¶ã•ã‚’å‘ä¸Š',
                'expected_score': 7.5,
                'use_case': 'æ—¥æœ¬èªãƒãƒ£ãƒƒãƒˆ',
                'difficulty': 'åˆç´š',
                'time': '20åˆ†'
            }
        ]
        
        for config in recommended_configs:
            with st.container():
                col1, col2, col3 = st.columns([3, 1, 1])
                
                with col1:
                    st.markdown(f"""
                    <div class="experiment-card">
                        <h4>{config['name']}</h4>
                        <p>{config['description']}</p>
                        <small>ğŸ’¡ ç”¨é€”: {config['use_case']} | â±ï¸ æ‰€è¦æ™‚é–“: {config['time']} | ğŸ“Š æœŸå¾…ã‚¹ã‚³ã‚¢: {config['expected_score']}/10</small>
                    </div>
                    """, unsafe_allow_html=True)
                
                with col2:
                    difficulty_color = {'åˆç´š': 'ğŸŸ¢', 'ä¸­ç´š': 'ğŸŸ¡', 'ä¸Šç´š': 'ğŸ”´'}
                    st.markdown(f"**é›£æ˜“åº¦**: {difficulty_color.get(config['difficulty'], 'ğŸ”˜')} {config['difficulty']}")
                
                with col3:
                    if st.button(f"â–¶ï¸ å®Ÿè¡Œ", key=f"run_{config['filename']}"):
                        run_experiment_flow(app, config['filename'])
    
    with tab2:
        st.subheader("âœï¸ ã‚«ã‚¹ã‚¿ãƒ å®Ÿé¨“è¨­å®š")
        
        # åŸºæœ¬è¨­å®š
        col1, col2 = st.columns(2)
        
        with col1:
            merge_method = st.selectbox(
                "ãƒãƒ¼ã‚¸æ‰‹æ³•",
                ["slerp", "evolutionary", "lora"],
                help="ãƒãƒ¼ã‚¸ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’é¸æŠ"
            )
            
            model1 = st.text_input(
                "ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ« 1",
                "google/gemma-3-4b-it",
                help="HuggingFace Hubä¸Šã®ãƒ¢ãƒ‡ãƒ«å"
            )
            
            weight1 = st.slider("ãƒ¢ãƒ‡ãƒ«1ã®é‡ã¿", 0.0, 1.0, 0.6, 0.1)
        
        with col2:
            output_name = st.text_input(
                "å‡ºåŠ›ãƒ¢ãƒ‡ãƒ«å",
                "my_custom_merge",
                help="ãƒãƒ¼ã‚¸å¾Œã®ãƒ¢ãƒ‡ãƒ«å"
            )
            
            model2 = st.text_input(
                "ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ« 2", 
                "Qwen/Qwen3-4B-Instruct"
            )
            
            weight2 = 1.0 - weight1
            st.metric("ãƒ¢ãƒ‡ãƒ«2ã®é‡ã¿", f"{weight2:.1f}")
        
        # è©•ä¾¡è¨­å®š
        st.subheader("ğŸ“Š è©•ä¾¡è¨­å®š")
        benchmarks = st.multiselect(
            "å®Ÿè¡Œã™ã‚‹ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯",
            ["mt-bench-jp", "jglue", "math"],
            default=["mt-bench-jp"]
        )
        
        # å®Ÿè¡Œãƒœã‚¿ãƒ³
        col1, col2 = st.columns(2)
        with col1:
            if st.button("ğŸš€ ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å®Ÿè¡Œ", type="primary"):
                # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç”Ÿæˆ
                custom_config = {
                    'merge_method': merge_method,
                    'output_path': f'models/{output_name}',
                    'models': [
                        {'name': model1, 'weight': weight1},
                        {'name': model2, 'weight': weight2}
                    ],
                    'alpha': weight1,
                    'evaluation': {
                        'benchmarks': benchmarks
                    }
                }
                
                # ä¸€æ™‚è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ
                temp_config_path = app.configs_dir / f"temp_{output_name}.yaml"
                with open(temp_config_path, 'w') as f:
                    yaml.dump(custom_config, f)
                
                run_experiment_with_realtime_logs(app, temp_config_path.name)
                
        with col2:
            if st.button("âš¡ ç°¡æ˜“å®Ÿè¡Œ"):
                # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç”Ÿæˆ
                custom_config = {
                    'merge_method': merge_method,
                    'output_path': f'models/{output_name}',
                    'models': [
                        {'name': model1, 'weight': weight1},
                        {'name': model2, 'weight': weight2}
                    ],
                    'alpha': weight1,
                    'evaluation': {
                        'benchmarks': benchmarks
                    }
                }
                
                # ä¸€æ™‚è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ
                temp_config_path = app.configs_dir / f"temp_{output_name}.yaml"
                with open(temp_config_path, 'w') as f:
                    yaml.dump(custom_config, f)
                
                run_experiment_flow(app, temp_config_path.name)


def show_experiment_results(app):
    """å®Ÿé¨“çµæœè©³ç´°è¡¨ç¤ºãƒ»ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆæ©Ÿèƒ½ä»˜ã"""
    st.markdown('<div class="section-title">ğŸ“ˆ å®Ÿé¨“çµæœè©³ç´°</div>', unsafe_allow_html=True)
    
    experiments = app.load_experiments()
    
    if not experiments:
        st.markdown("""
        <div class="dashboard-section">
            <p style="text-align: center; color: #64748b; font-size: 1.1rem;">
            ğŸ“Š å®Ÿé¨“çµæœãŒã‚ã‚Šã¾ã›ã‚“ã€‚æ–°ã—ã„å®Ÿé¨“ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚
            </p>
        </div>
        """, unsafe_allow_html=True)
        return
    
    # ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆæ©Ÿèƒ½
    col1, col2, col3 = st.columns([2, 1, 1])
    with col1:
        st.markdown("### ğŸ“Š å®Ÿé¨“ãƒ‡ãƒ¼ã‚¿ã®ç®¡ç†")
    with col2:
        if st.button("ğŸ“ JSON ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ", use_container_width=True):
            export_experiments_json(experiments)
    with col3:
        if st.button("ğŸ“Š CSV ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ", use_container_width=True):
            export_experiments_csv(experiments)
    
    # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
    col1, col2, col3 = st.columns(3)
    
    with col1:
        status_filter = st.selectbox(
            "ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã§ãƒ•ã‚£ãƒ«ã‚¿",
            ["ã™ã¹ã¦", "completed", "running", "failed"]
        )
    
    with col2:
        method_filter = st.selectbox(
            "æ‰‹æ³•ã§ãƒ•ã‚£ãƒ«ã‚¿",
            ["ã™ã¹ã¦"] + list(set([e.get('merge_method', '') for e in experiments]))
        )
    
    with col3:
        sort_by = st.selectbox(
            "ã‚½ãƒ¼ãƒˆé †",
            ["æ—¥æ™‚(æ–°ã—ã„é †)", "æ—¥æ™‚(å¤ã„é †)", "ã‚¹ã‚³ã‚¢(é«˜ã„é †)", "ã‚¹ã‚³ã‚¢(ä½ã„é †)"]
        )
    
    # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°é©ç”¨
    filtered_experiments = experiments
    
    if status_filter != "ã™ã¹ã¦":
        filtered_experiments = [e for e in filtered_experiments if e.get('status') == status_filter]
    
    if method_filter != "ã™ã¹ã¦":
        filtered_experiments = [e for e in filtered_experiments if e.get('merge_method') == method_filter]
    
    # ã‚½ãƒ¼ãƒˆ
    if sort_by == "æ—¥æ™‚(æ–°ã—ã„é †)":
        filtered_experiments.sort(key=lambda x: x.get('timestamp', ''), reverse=True)
    elif sort_by == "æ—¥æ™‚(å¤ã„é †)":
        filtered_experiments.sort(key=lambda x: x.get('timestamp', ''))
    elif sort_by == "ã‚¹ã‚³ã‚¢(é«˜ã„é †)":
        filtered_experiments.sort(
            key=lambda x: x.get('evaluations', {}).get('mt_bench_jp', {}).get('overall_score', 0), 
            reverse=True
        )
    elif sort_by == "ã‚¹ã‚³ã‚¢(ä½ã„é †)":
        filtered_experiments.sort(
            key=lambda x: x.get('evaluations', {}).get('mt_bench_jp', {}).get('overall_score', 0)
        )
    
    # ã‚¿ãƒ–ã§çµæœè¡¨ç¤ºã¨ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆãƒ»æ¯”è¼ƒæ©Ÿèƒ½ã‚’åˆ†é›¢
    tab1, tab2 = st.tabs(["ğŸ“Š å®Ÿé¨“çµæœä¸€è¦§", "ğŸ” å®Ÿé¨“æ¯”è¼ƒ"])
    
    with tab1:
        # çµæœè¡¨ç¤º
        for experiment in filtered_experiments:
            show_detailed_experiment_card(experiment)
    
    with tab2:
        # å®Ÿé¨“æ¯”è¼ƒæ©Ÿèƒ½
        show_experiment_comparison(filtered_experiments)


def show_config_management(app):
    """è¨­å®šç®¡ç†ç”»é¢"""
    st.header("âš™ï¸ è¨­å®šç®¡ç†")
    
    configs = app.load_configs()
    
    tab1, tab2 = st.tabs(["ğŸ“‹ æ—¢å­˜è¨­å®š", "â• æ–°è¦ä½œæˆ"])
    
    with tab1:
        st.subheader("ğŸ“‹ åˆ©ç”¨å¯èƒ½ãªè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«")
        
        for config in configs:
            with st.expander(f"ğŸ“„ {config['filename']}"):
                col1, col2 = st.columns([3, 1])
                
                with col1:
                    st.code(yaml.dump(config, default_flow_style=False), language='yaml')
                
                with col2:
                    if st.button("âœï¸ ç·¨é›†", key=f"edit_{config['filename']}"):
                        st.session_state[f"editing_{config['filename']}"] = True
                    
                    if st.button("ğŸ—‘ï¸ å‰Šé™¤", key=f"delete_{config['filename']}"):
                        try:
                            os.remove(app.configs_dir / config['filename'])
                            st.success("è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤ã—ã¾ã—ãŸ")
                            st.rerun()
                        except Exception as e:
                            st.error(f"å‰Šé™¤ã«å¤±æ•—: {e}")
    
    with tab2:
        st.subheader("â• æ–°ã—ã„è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ")
        
        # ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆé¸æŠ
        template = st.selectbox(
            "ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’é¸æŠ",
            ["ç©ºã®è¨­å®š", "SLERPè¨­å®š", "LoRAè¨­å®š", "Evolutionaryè¨­å®š"]
        )
        
        filename = st.text_input("ãƒ•ã‚¡ã‚¤ãƒ«å", "my_config.yaml")
        
        # è¨­å®šã‚¨ãƒ‡ã‚£ã‚¿
        if template == "SLERPè¨­å®š":
            default_config = """merge_method: slerp
output_path: models/my_merged_model

models:
  - name: model1/name
    weight: 0.6
  - name: model2/name
    weight: 0.4

alpha: 0.6

evaluation:
  benchmarks:
    - mt-bench-jp
"""
        else:
            default_config = """merge_method: slerp
output_path: models/new_model

models:
  - name: model1
  - name: model2
"""
        
        config_text = st.text_area(
            "è¨­å®šå†…å®¹ (YAML)", 
            default_config, 
            height=400
        )
        
        if st.button("ğŸ’¾ ä¿å­˜"):
            try:
                # YAMLå½¢å¼ãƒã‚§ãƒƒã‚¯
                yaml.safe_load(config_text)
                
                # ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
                with open(app.configs_dir / filename, 'w') as f:
                    f.write(config_text)
                
                st.success(f"è¨­å®šãƒ•ã‚¡ã‚¤ãƒ« {filename} ã‚’ä¿å­˜ã—ã¾ã—ãŸ")
                st.rerun()
                
            except yaml.YAMLError as e:
                st.error(f"YAMLå½¢å¼ã‚¨ãƒ©ãƒ¼: {e}")
            except Exception as e:
                st.error(f"ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")


def show_guide():
    """ã‚¬ã‚¤ãƒ‰è¡¨ç¤º"""
    st.header("ğŸ“š AI Fusion Studio ã‚¬ã‚¤ãƒ‰")
    
    tab1, tab2, tab3, tab4 = st.tabs(["ğŸš€ ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆ", "ğŸ”§ ä½¿ã„æ–¹", "ğŸ’¡ Tips", "â“ FAQ"])
    
    with tab1:
        st.markdown("""
        ## ğŸš€ 5åˆ†ã§å§‹ã‚ã‚‹AIèåˆ
        
        ### 1. æ¨å¥¨å®Ÿé¨“ã‹ã‚‰å§‹ã‚ã‚‹
        
        **Gemma Ã— Qwen SLERP** ãŒãŠã™ã™ã‚ã§ã™ï¼š
        - ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ã§å®Ÿè¨¼æ¸ˆã¿ï¼ˆMT-Bench 8.5/10ï¼‰
        - å®Ÿè£…ãŒç°¡å˜
        - æ±ç”¨çš„ãªæ—¥æœ¬èªã‚¿ã‚¹ã‚¯ã«æœ€é©
        
        ### 2. ã‚µã‚¤ãƒ‰ãƒãƒ¼ã‹ã‚‰ã€Œæ¨å¥¨å®Ÿé¨“å®Ÿè¡Œã€ã‚’ã‚¯ãƒªãƒƒã‚¯
        
        ### 3. çµæœã‚’ç¢ºèª
        - ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã§é€²æ—ã‚’ãƒã‚§ãƒƒã‚¯
        - å®Œäº†å¾Œã¯å®Ÿé¨“çµæœã‚¿ãƒ–ã§è©³ç´°ã‚’ç¢ºèª
        
        ### 4. ä»–ã®çµ„ã¿åˆã‚ã›ã‚‚è©¦ã™
        - æ•°ç†æ¨è«–å¼·åŒ–ãªã‚‰ã€ŒEvoLLM-JPå†ç¾ã€
        - æ—¥æœ¬èªãƒãƒ£ãƒƒãƒˆæ”¹å–„ãªã‚‰ã€ŒGemma + Swallow LoRAã€
        """)
    
    with tab2:
        st.markdown("""
        ## ğŸ”§ è©³ç´°ãªä½¿ã„æ–¹
        
        ### ãƒãƒ¼ã‚¸æ‰‹æ³•
        
        **SLERPï¼ˆçƒé¢ç·šå½¢è£œé–“ï¼‰**
        - 2ã¤ã®ãƒ¢ãƒ‡ãƒ«ã‚’æ»‘ã‚‰ã‹ã«è£œé–“
        - å®‰å®šã—ãŸçµæœãŒæœŸå¾…ã§ãã‚‹
        - æ¨å¥¨ï¼šåˆå¿ƒè€…å‘ã‘
        
        **Evolutionary Merge**
        - éºä¼çš„ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã§æœ€é©åŒ–
        - é«˜æ€§èƒ½ã ãŒæ™‚é–“ãŒã‹ã‹ã‚‹
        - æ¨å¥¨ï¼šä¸­ç´šè€…å‘ã‘
        
        **LoRAçµ±åˆ**
        - æ—¢å­˜ãƒ¢ãƒ‡ãƒ«ã«è»½é‡ãªæ”¹å–„ã‚’è¿½åŠ 
        - é«˜é€Ÿã§å®‰å…¨
        - æ¨å¥¨ï¼šç‰¹å®šæ©Ÿèƒ½ã®æ”¹å–„
        
        ### è©•ä¾¡æŒ‡æ¨™
        
        **MT-Bench JP**
        - æ—¥æœ¬èªã‚¿ã‚¹ã‚¯ã®ç·åˆè©•ä¾¡
        - 0-10ã®ã‚¹ã‚³ã‚¢
        - 7.0ä»¥ä¸Šã§å®Ÿç”¨ãƒ¬ãƒ™ãƒ«
        
        **æ•°ç†æ¨è«–**
        - æ•°å­¦å•é¡Œã®æ­£ç­”ç‡
        - è«–ç†çš„æ€è€ƒèƒ½åŠ›ã‚’æ¸¬å®š
        """)
    
    with tab3:
        st.markdown("""
        ## ğŸ’¡ æˆåŠŸã®ãŸã‚ã®Tips
        
        ### 1. ãƒ¢ãƒ‡ãƒ«é¸æŠ
        - **æ—¥æœ¬èªå¼·åŒ–**: æµ·å¤–ãƒ¢ãƒ‡ãƒ« + æ—¥æœ¬èªç‰¹åŒ–ãƒ¢ãƒ‡ãƒ«
        - **æ¨è«–å¼·åŒ–**: ãƒãƒ£ãƒƒãƒˆãƒ¢ãƒ‡ãƒ« + æ•°ç†ç‰¹åŒ–ãƒ¢ãƒ‡ãƒ«
        - **ãƒãƒ©ãƒ³ã‚¹é‡è¦–**: åŒã‚µã‚¤ã‚ºã®ç•°ç³»åˆ—ãƒ¢ãƒ‡ãƒ«
        
        ### 2. é‡ã¿é…åˆ†
        - **0.6:0.4** - ãƒãƒ©ãƒ³ã‚¹ãŒè‰¯ã„
        - **0.7:0.3** - ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«é‡è¦–
        - **0.5:0.5** - å‡ç­‰é…åˆ†
        
        ### 3. é‡å­åŒ–
        - **AWQ 4bit** - å“è³ªã¨åŠ¹ç‡ã®ãƒãƒ©ãƒ³ã‚¹
        - **2bit** - æœ€å¤§åœ§ç¸®ï¼ˆå“è³ªä½ä¸‹ã‚ã‚Šï¼‰
        - **GGUF** - CPUå®Ÿè¡Œå‘ã‘
        
        ### 4. ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°
        - ãƒ¡ãƒ¢ãƒªä¸è¶³ â†’ CPUå®Ÿè¡Œã«åˆ‡ã‚Šæ›¿ãˆ
        - ä½ã‚¹ã‚³ã‚¢ â†’ é‡ã¿é…åˆ†ã‚’èª¿æ•´
        - ã‚¨ãƒ©ãƒ¼ â†’ ãƒ¢ãƒ‡ãƒ«ã®äº’æ›æ€§ç¢ºèª
        """)
    
    with tab4:
        st.markdown("""
        ## â“ ã‚ˆãã‚ã‚‹è³ªå•
        
        **Q: ã©ã®ãã‚‰ã„æ™‚é–“ãŒã‹ã‹ã‚Šã¾ã™ã‹ï¼Ÿ**
        A: 
        - SLERP: 20-30åˆ†
        - LoRA: 10-20åˆ†  
        - Evolutionary: 60-90åˆ†
        
        **Q: å¿…è¦ãªãƒ¡ãƒ¢ãƒªã¯ï¼Ÿ**
        A:
        - 4Bãƒ¢ãƒ‡ãƒ«: 8GBä»¥ä¸Šæ¨å¥¨
        - 7Bãƒ¢ãƒ‡ãƒ«: 16GBä»¥ä¸Šæ¨å¥¨
        - CPUå®Ÿè¡Œã‚‚å¯èƒ½ï¼ˆæ™‚é–“ã¯ã‹ã‹ã‚Šã¾ã™ï¼‰
        
        **Q: å•†ç”¨åˆ©ç”¨ã§ãã¾ã™ã‹ï¼Ÿ**
        A: å„ãƒ¢ãƒ‡ãƒ«ã®ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚
        
        **Q: ã‚«ã‚¹ã‚¿ãƒ ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ã„ãŸã„**
        A: HuggingFace Hubä¸Šã®ä»»æ„ã®ãƒ¢ãƒ‡ãƒ«ãŒä½¿ç”¨å¯èƒ½ã§ã™ã€‚
        
        **Q: å®Ÿé¨“ãŒå¤±æ•—ã—ã¾ã—ãŸ**
        A: 
        1. ãƒ¢ãƒ‡ãƒ«åã®ã‚¹ãƒšãƒ«ãƒã‚§ãƒƒã‚¯
        2. ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆæ¥ç¶šç¢ºèª
        3. ãƒ¡ãƒ¢ãƒªä¸è¶³ã®å ´åˆã¯CPUå®Ÿè¡Œã‚’è©¦ã™
        """)


def run_experiment_flow(app, config_filename):
    """å®Ÿé¨“å®Ÿè¡Œãƒ•ãƒ­ãƒ¼"""
    with st.spinner(f"å®Ÿé¨“ã‚’å®Ÿè¡Œä¸­: {config_filename}"):
        # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        # ã‚¹ãƒ†ãƒƒãƒ—ã”ã¨ã®å®Ÿè¡Œ
        steps = [
            ("ãƒ¢ãƒ‡ãƒ«æ¤œè¨¼", 20),
            ("ãƒ¢ãƒ‡ãƒ«ãƒãƒ¼ã‚¸", 40), 
            ("è©•ä¾¡å®Ÿè¡Œ", 70),
            ("é‡å­åŒ–", 90),
            ("çµæœä¿å­˜", 100)
        ]
        
        for step_name, progress in steps:
            status_text.text(f"ğŸ”„ {step_name}ä¸­...")
            progress_bar.progress(progress)
            
            # å®Ÿéš›ã®å‡¦ç†æ™‚é–“ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
            import time
            time.sleep(1)
        
        # å®Ÿéš›ã®å®Ÿé¨“å®Ÿè¡Œ
        success, stdout, stderr = app.run_experiment(f"configs/{config_filename}")
        
        if success:
            st.success("âœ… å®Ÿé¨“ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
            st.info("çµæœã¯ã€Œå®Ÿé¨“çµæœã€ã‚¿ãƒ–ã§ç¢ºèªã§ãã¾ã™ã€‚")
        else:
            st.error(f"âŒ å®Ÿé¨“ãŒå¤±æ•—ã—ã¾ã—ãŸ: {stderr}")
            with st.expander("è©³ç´°ã‚¨ãƒ©ãƒ¼"):
                st.code(stderr)


def show_experiment_card(experiment):
    """å®Ÿé¨“ã‚«ãƒ¼ãƒ‰ã‚’è¡¨ç¤º"""
    status_class = f"status-{experiment.get('status', 'unknown')}"
    
    col1, col2, col3 = st.columns([3, 1, 1])
    
    with col1:
        st.markdown(f"**{experiment.get('id', 'Unknown')}**")
        st.text(f"æ‰‹æ³•: {experiment.get('merge_method', 'N/A')}")
        
    with col2:
        status = experiment.get('status', 'unknown')
        st.markdown(f'<span class="status-badge {status_class}">{status}</span>', 
                   unsafe_allow_html=True)
    
    with col3:
        if 'evaluations' in experiment:
            score = experiment['evaluations'].get('mt_bench_jp', {}).get('overall_score', 0)
            st.metric("MT-Bench", f"{score:.2f}")


def show_modern_experiment_card(experiment):
    """ãƒ¢ãƒ€ãƒ³ãªå®Ÿé¨“ã‚«ãƒ¼ãƒ‰è¡¨ç¤º"""
    exp_id = experiment.get('id', 'Unknown')
    method = experiment.get('merge_method', 'N/A')
    status = experiment.get('status', 'unknown')
    
    # ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã«å¿œã˜ãŸè‰²ã‚’è¨­å®š
    status_colors = {
        'completed': '#10b981',
        'running': '#f59e0b', 
        'failed': '#ef4444',
        'unknown': '#6b7280'
    }
    
    status_color = status_colors.get(status, '#6b7280')
    
    # ã‚¹ã‚³ã‚¢å–å¾—
    score = 0
    if 'evaluations' in experiment:
        score = experiment['evaluations'].get('mt_bench_jp', {}).get('overall_score', 0)
    
    # ãƒ¢ãƒ€ãƒ³ãªã‚«ãƒ¼ãƒ‰ãƒ‡ã‚¶ã‚¤ãƒ³
    st.markdown(f"""
    <div class="recent-experiment">
        <div style="display: flex; justify-content: space-between; align-items: center; margin-bottom: 1rem;">
            <div>
                <h4 style="margin: 0; color: #1e293b; font-size: 1.2rem;">{exp_id}</h4>
                <p style="margin: 0.25rem 0 0 0; color: #64748b; font-size: 0.95rem;">æ‰‹æ³•: <strong>{method}</strong></p>
            </div>
            <div style="text-align: right;">
                <span style="background: {status_color}; color: white; padding: 0.25rem 0.75rem; 
                           border-radius: 12px; font-size: 0.85rem; font-weight: 600; text-transform: uppercase;">
                    {status}
                </span>
            </div>
        </div>
        
        <div style="display: flex; justify-content: space-between; align-items: center;">
            <div style="color: #64748b; font-size: 0.9rem;">
                {experiment.get('timestamp', 'N/A')[:16] if experiment.get('timestamp') else 'N/A'}
            </div>
            {f'<div class="score-highlight">MT-Bench: {score:.2f}</div>' if score > 0 else '<div style="color: #94a3b8;">ã‚¹ã‚³ã‚¢æœªæ¸¬å®š</div>'}
        </div>
    </div>
    """, unsafe_allow_html=True)


def show_detailed_experiment_card(experiment):
    """è©³ç´°å®Ÿé¨“ã‚«ãƒ¼ãƒ‰ã‚’è¡¨ç¤º"""
    with st.expander(f"ğŸ“Š {experiment.get('id', 'Unknown')} - {experiment.get('merge_method', 'N/A')}"):
        col1, col2 = st.columns(2)
        
        with col1:
            st.subheader("åŸºæœ¬æƒ…å ±")
            st.write(f"**å®Ÿé¨“ID**: {experiment.get('id', 'N/A')}")
            st.write(f"**æ‰‹æ³•**: {experiment.get('merge_method', 'N/A')}")
            st.write(f"**ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹**: {experiment.get('status', 'N/A')}")
            st.write(f"**å®Ÿè¡Œæ—¥æ™‚**: {experiment.get('timestamp', 'N/A')}")
            
            if 'models' in experiment:
                st.subheader("ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«")
                for model in experiment['models']:
                    st.write(f"- {model.get('name', 'N/A')} (é‡ã¿: {model.get('weight', 'N/A')})")
        
        with col2:
            if 'evaluations' in experiment:
                st.subheader("è©•ä¾¡çµæœ")
                evals = experiment['evaluations']
                
                if 'mt_bench_jp' in evals:
                    mt_score = evals['mt_bench_jp'].get('overall_score', 0)
                    st.metric("MT-Benchç·åˆ", f"{mt_score:.2f}/10")
                    
                    if 'category_scores' in evals['mt_bench_jp']:
                        st.write("**ã‚«ãƒ†ã‚´ãƒªåˆ¥ã‚¹ã‚³ã‚¢**:")
                        for cat, score in evals['mt_bench_jp']['category_scores'].items():
                            st.write(f"- {cat}: {score:.2f}")
                
                if 'mathematical_reasoning' in evals:
                    math_acc = evals['mathematical_reasoning'].get('accuracy', 0)
                    st.metric("æ•°ç†æ¨è«–ç²¾åº¦", f"{math_acc:.2%}")
            
            if 'quantization' in experiment:
                st.subheader("é‡å­åŒ–æƒ…å ±")
                quant = experiment['quantization']
                st.write(f"**æ‰‹æ³•**: {quant.get('method', 'N/A')}")
                st.write(f"**ã‚µã‚¤ã‚º**: {quant.get('quantized_size_gb', 'N/A'):.2f} GB")
                st.write(f"**åœ§ç¸®ç‡**: {quant.get('compression_ratio', 'N/A'):.2f}x")


def create_experiments_dataframe(experiments):
    """å®Ÿé¨“ãƒ‡ãƒ¼ã‚¿ã‚’DataFrameã«å¤‰æ›"""
    data = []
    for exp in experiments:
        if exp.get('status') == 'completed':
            row = {
                'experiment_id': exp.get('id', ''),
                'method': exp.get('merge_method', ''),
                'timestamp': exp.get('timestamp', ''),
                'status': exp.get('status', ''),
                # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’è¨­å®š
                'mt_bench_score': 0,
                'math_accuracy': 0,
                'model_size_gb': 0,
                'compression_ratio': 0
            }
            
            if 'evaluations' in exp:
                if 'mt_bench_jp' in exp['evaluations']:
                    row['mt_bench_score'] = exp['evaluations']['mt_bench_jp'].get('overall_score', 0)
                if 'mathematical_reasoning' in exp['evaluations']:
                    row['math_accuracy'] = exp['evaluations']['mathematical_reasoning'].get('accuracy', 0)
            
            if 'quantization' in exp:
                row['model_size_gb'] = exp['quantization'].get('quantized_size_gb', 0)
                row['compression_ratio'] = exp['quantization'].get('compression_ratio', 0)
            
            data.append(row)
    
    # ç©ºã®DataFrameã§ã‚‚å¿…è¦ãªåˆ—ã‚’æŒã¤ã‚ˆã†ã«ã™ã‚‹
    if not data:
        return pd.DataFrame(columns=['experiment_id', 'method', 'timestamp', 'status', 
                                    'mt_bench_score', 'math_accuracy', 'model_size_gb', 
                                    'compression_ratio'])
    
    return pd.DataFrame(data)


def export_experiments_json(experiments):
    """å®Ÿé¨“ãƒ‡ãƒ¼ã‚¿ã‚’JSONã§ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ"""
    import json
    from datetime import datetime
    
    export_data = {
        'export_timestamp': datetime.now().isoformat(),
        'total_experiments': len(experiments),
        'experiments': experiments
    }
    
    json_str = json.dumps(export_data, indent=2, ensure_ascii=False)
    
    st.download_button(
        label="ğŸ“ JSON ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
        data=json_str,
        file_name=f"llm_experiments_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json",
        mime="application/json"
    )
    st.success("JSONã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã®æº–å‚™ãŒå®Œäº†ã—ã¾ã—ãŸï¼")


def export_experiments_csv(experiments):
    """å®Ÿé¨“ãƒ‡ãƒ¼ã‚¿ã‚’CSVã§ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ"""
    from datetime import datetime
    
    df = create_experiments_dataframe(experiments)
    
    # ã‚ˆã‚Šè©³ç´°ãªCSVãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆ
    detailed_data = []
    for exp in experiments:
        row = {
            'experiment_id': exp.get('id', ''),
            'method': exp.get('merge_method', ''),
            'timestamp': exp.get('timestamp', ''),
            'status': exp.get('status', ''),
            'mt_bench_score': 0,
            'math_accuracy': 0,
            'model_size_gb': 0,
            'compression_ratio': 0,
            'models_used': '',
            'output_path': exp.get('output_path', ''),
            'description': exp.get('description', '')
        }
        
        # è©•ä¾¡çµæœ
        if 'evaluations' in exp:
            if 'mt_bench_jp' in exp['evaluations']:
                row['mt_bench_score'] = exp['evaluations']['mt_bench_jp'].get('overall_score', 0)
            if 'mathematical_reasoning' in exp['evaluations']:
                row['math_accuracy'] = exp['evaluations']['mathematical_reasoning'].get('accuracy', 0)
        
        # é‡å­åŒ–æƒ…å ±
        if 'quantization' in exp:
            row['model_size_gb'] = exp['quantization'].get('quantized_size_gb', 0)
            row['compression_ratio'] = exp['quantization'].get('compression_ratio', 0)
        
        # ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«
        if 'models' in exp:
            models_list = [f"{m.get('name', '')}({m.get('weight', '')})" for m in exp['models']]
            row['models_used'] = '; '.join(models_list)
        
        detailed_data.append(row)
    
    detailed_df = pd.DataFrame(detailed_data)
    csv_str = detailed_df.to_csv(index=False)
    
    st.download_button(
        label="ğŸ“Š CSV ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
        data=csv_str,
        file_name=f"llm_experiments_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
        mime="text/csv"
    )
    st.success("CSVã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã®æº–å‚™ãŒå®Œäº†ã—ã¾ã—ãŸï¼")


def run_experiment_with_realtime_logs(app, config_filename):
    """ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ­ã‚°ä»˜ãå®Ÿé¨“å®Ÿè¡Œ"""
    import subprocess
    import time
    import threading
    import queue
    
    st.markdown("### ğŸš€ å®Ÿé¨“å®Ÿè¡Œä¸­...")
    
    # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ã¨ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹
    progress_bar = st.progress(0)
    status_container = st.empty()
    log_container = st.empty()
    
    # ãƒ­ã‚°ã‚­ãƒ¥ãƒ¼ã‚’ä½œæˆ
    log_queue = queue.Queue()
    
    def run_experiment():
        """å®Ÿé¨“ã‚’å®Ÿéš›ã«å®Ÿè¡Œã™ã‚‹é–¢æ•°"""
        try:
            # å®Ÿéš›ã®å®Ÿé¨“ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’å®Ÿè¡Œ
            process = subprocess.Popen(
                ["python", "scripts/run_experiment.py", f"configs/{config_filename}"],
                stdout=subprocess.PIPE,
                stderr=subprocess.STDOUT,
                universal_newlines=True,
                cwd=str(Path(__file__).parent.parent)
            )
            
            # å‡ºåŠ›ã‚’é€æ¬¡å–å¾—
            for line in iter(process.stdout.readline, ''):
                log_queue.put(('log', line.strip()))
                
            process.wait()
            log_queue.put(('done', process.returncode))
            
        except Exception as e:
            log_queue.put(('error', str(e)))
    
    # å®Ÿé¨“ã‚’åˆ¥ã‚¹ãƒ¬ãƒƒãƒ‰ã§é–‹å§‹
    experiment_thread = threading.Thread(target=run_experiment)
    experiment_thread.start()
    
    # ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ­ã‚°è¡¨ç¤º
    logs = []
    steps = [
        ("ãƒ¢ãƒ‡ãƒ«æ¤œè¨¼", 10),
        ("ãƒ¢ãƒ‡ãƒ«ãƒãƒ¼ã‚¸", 40),
        ("è©•ä¾¡å®Ÿè¡Œ", 70),
        ("é‡å­åŒ–", 90),
        ("çµæœä¿å­˜", 100)
    ]
    current_step = 0
    
    while experiment_thread.is_alive() or not log_queue.empty():
        try:
            # ãƒ­ã‚°ã‚’ãƒã‚§ãƒƒã‚¯
            log_type, content = log_queue.get(timeout=0.1)
            
            if log_type == 'log':
                logs.append(content)
                
                # ã‚¹ãƒ†ãƒƒãƒ—é€²è¡Œã‚’åˆ¤å®š
                if current_step < len(steps):
                    step_name, progress = steps[current_step]
                    status_container.text(f"ğŸ”„ {step_name}ä¸­...")
                    progress_bar.progress(progress)
                    current_step += 1
                
                # ãƒ­ã‚°è¡¨ç¤ºï¼ˆæœ€æ–°10è¡Œï¼‰
                recent_logs = logs[-10:]
                log_text = '\n'.join(recent_logs)
                log_container.text_area("ğŸ“‹ å®Ÿè¡Œãƒ­ã‚°", log_text, height=200)
                
            elif log_type == 'done':
                if content == 0:
                    progress_bar.progress(100)
                    status_container.success("âœ… å®Ÿé¨“ãŒæ­£å¸¸ã«å®Œäº†ã—ã¾ã—ãŸï¼")
                else:
                    status_container.error(f"âŒ å®Ÿé¨“ãŒå¤±æ•—ã—ã¾ã—ãŸ (ã‚¨ãƒ©ãƒ¼ã‚³ãƒ¼ãƒ‰: {content})")
                break
                
            elif log_type == 'error':
                status_container.error(f"âŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {content}")
                break
                
        except queue.Empty:
            continue
        except Exception as e:
            status_container.error(f"âŒ äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {e}")
            break
    
    experiment_thread.join()


def show_experiment_comparison(experiments):
    """å®Ÿé¨“æ¯”è¼ƒæ©Ÿèƒ½"""
    st.markdown('<div class="section-title">ğŸ” å®Ÿé¨“æ¯”è¼ƒ</div>', unsafe_allow_html=True)
    
    if len(experiments) < 2:
        st.info("æ¯”è¼ƒã™ã‚‹ã«ã¯2ã¤ä»¥ä¸Šã®å®Ÿé¨“ãŒå¿…è¦ã§ã™ã€‚")
        return
    
    # æ¯”è¼ƒã™ã‚‹å®Ÿé¨“ã‚’é¸æŠ
    col1, col2 = st.columns(2)
    
    with col1:
        exp1_options = [f"{exp.get('id', 'Unknown')} ({exp.get('merge_method', 'N/A')})" 
                      for exp in experiments]
        selected_exp1 = st.selectbox("å®Ÿé¨“1ã‚’é¸æŠ", exp1_options, key="exp1")
        
    with col2:
        selected_exp2 = st.selectbox("å®Ÿé¨“2ã‚’é¸æŠ", exp1_options, key="exp2")
    
    if selected_exp1 != selected_exp2:
        exp1 = experiments[exp1_options.index(selected_exp1)]
        exp2 = experiments[exp1_options.index(selected_exp2)]
        
        # æ¯”è¼ƒãƒ†ãƒ¼ãƒ–ãƒ«
        comparison_data = []
        
        metrics = [
            ("å®Ÿé¨“ID", "id", ""),
            ("æ‰‹æ³•", "merge_method", ""),
            ("ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹", "status", ""),
            ("MT-Benchã‚¹ã‚³ã‚¢", "evaluations.mt_bench_jp.overall_score", 0),
            ("æ•°å­¦ç²¾åº¦", "evaluations.mathematical_reasoning.accuracy", 0),
            ("ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚º(GB)", "quantization.quantized_size_gb", 0),
            ("åœ§ç¸®ç‡", "quantization.compression_ratio", 0)
        ]
        
        for metric_name, path, default in metrics:
            value1 = get_nested_value(exp1, path, default)
            value2 = get_nested_value(exp2, path, default)
            comparison_data.append({
                "é …ç›®": metric_name,
                "å®Ÿé¨“1": value1,
                "å®Ÿé¨“2": value2,
                "å·®åˆ†": calculate_diff(value1, value2)
            })
        
        df_comparison = pd.DataFrame(comparison_data)
        st.dataframe(df_comparison, use_container_width=True)
        
        # è¦–è¦šçš„æ¯”è¼ƒ
        if all(get_nested_value(exp, "evaluations.mt_bench_jp.overall_score", 0) > 0 for exp in [exp1, exp2]):
            col1, col2 = st.columns(2)
            
            with col1:
                st.markdown("#### ğŸ“Š MT-Benchã‚¹ã‚³ã‚¢æ¯”è¼ƒ")
                scores = [
                    get_nested_value(exp1, "evaluations.mt_bench_jp.overall_score", 0),
                    get_nested_value(exp2, "evaluations.mt_bench_jp.overall_score", 0)
                ]
                labels = [exp1.get('id', 'Exp1'), exp2.get('id', 'Exp2')]
                
                fig = px.bar(x=labels, y=scores, title="MT-Benchã‚¹ã‚³ã‚¢æ¯”è¼ƒ")
                st.plotly_chart(fig, use_container_width=True)


def get_nested_value(data, path, default=None):
    """ãƒã‚¹ãƒˆã•ã‚ŒãŸè¾æ›¸ã‹ã‚‰å€¤ã‚’å–å¾—"""
    keys = path.split('.')
    current = data
    
    for key in keys:
        if isinstance(current, dict) and key in current:
            current = current[key]
        else:
            return default
    
    return current


def calculate_diff(val1, val2):
    """2ã¤ã®å€¤ã®å·®åˆ†ã‚’è¨ˆç®—"""
    if isinstance(val1, (int, float)) and isinstance(val2, (int, float)):
        diff = val2 - val1
        return f"{diff:+.2f}" if abs(diff) > 0.01 else "0.00"
    else:
        return "N/A" if val1 == val2 else "ç•°ãªã‚‹"


def show_data_management(app):
    """ãƒ‡ãƒ¼ã‚¿ç®¡ç†ãƒšãƒ¼ã‚¸"""
    st.markdown('<div class="section-title">ğŸ“ ãƒ‡ãƒ¼ã‚¿ç®¡ç†</div>', unsafe_allow_html=True)
    
    experiments = app.load_experiments()
    
    # çµ±è¨ˆæƒ…å ±
    st.markdown('<div class="dashboard-section">', unsafe_allow_html=True)
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric("ç·å®Ÿé¨“æ•°", len(experiments))
    with col2:
        completed = len([e for e in experiments if e.get('status') == 'completed'])
        st.metric("å®Œäº†å®Ÿé¨“", completed)
    with col3:
        failed = len([e for e in experiments if e.get('status') == 'failed'])
        st.metric("å¤±æ•—å®Ÿé¨“", failed)
    with col4:
        total_size = sum([e.get('quantization', {}).get('quantized_size_gb', 0) for e in experiments])
        st.metric("ç·ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º", f"{total_size:.1f}GB")
    
    st.markdown('</div>', unsafe_allow_html=True)
    
    # ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆãƒ»ã‚¤ãƒ³ãƒãƒ¼ãƒˆæ©Ÿèƒ½
    st.markdown('<div class="dashboard-section">', unsafe_allow_html=True)
    st.markdown("### ğŸ“¤ ãƒ‡ãƒ¼ã‚¿ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ")
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        if st.button("ğŸ“ JSON ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ", use_container_width=True):
            export_experiments_json(experiments)
    
    with col2:
        if st.button("ğŸ“Š CSV ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ", use_container_width=True):
            export_experiments_csv(experiments)
    
    with col3:
        if st.button("ğŸ“‹ ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ", use_container_width=True):
            generate_comprehensive_report(experiments)
    
    st.markdown('</div>', unsafe_allow_html=True)
    
    # ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
    st.markdown('<div class="dashboard-section">', unsafe_allow_html=True)
    st.markdown("### ğŸ§¹ ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—")
    
    col1, col2 = st.columns(2)
    
    with col1:
        if st.button("ğŸ—‘ï¸ å¤±æ•—å®Ÿé¨“ã‚’å‰Šé™¤", use_container_width=True):
            cleanup_failed_experiments(app, experiments)
    
    with col2:
        if st.button("ğŸ”„ å®Ÿé¨“ãƒ‡ãƒ¼ã‚¿æœ€é©åŒ–", use_container_width=True):
            optimize_experiment_data(app, experiments)
    
    st.markdown('</div>', unsafe_allow_html=True)
    
    # å®Ÿé¨“ãƒ‡ãƒ¼ã‚¿è©³ç´°
    if experiments:
        st.markdown('<div class="dashboard-section">', unsafe_allow_html=True)
        st.markdown("### ğŸ“Š å®Ÿé¨“ãƒ‡ãƒ¼ã‚¿è©³ç´°")
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ è¡¨ç¤º
        df = create_experiments_dataframe(experiments)
        st.dataframe(df, use_container_width=True)
        
        st.markdown('</div>', unsafe_allow_html=True)


def generate_comprehensive_report(experiments):
    """åŒ…æ‹¬çš„ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
    from datetime import datetime
    import json
    
    # ãƒ¬ãƒãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿ä½œæˆ
    report = {
        'generation_timestamp': datetime.now().isoformat(),
        'summary': {
            'total_experiments': len(experiments),
            'completed_experiments': len([e for e in experiments if e.get('status') == 'completed']),
            'failed_experiments': len([e for e in experiments if e.get('status') == 'failed']),
            'average_mt_bench_score': 0,
            'best_experiment': None,
            'worst_experiment': None
        },
        'detailed_analysis': {},
        'recommendations': []
    }
    
    # å®Œäº†å®Ÿé¨“ã®åˆ†æ
    completed_experiments = [e for e in experiments if e.get('status') == 'completed']
    if completed_experiments:
        scores = []
        for exp in completed_experiments:
            score = exp.get('evaluations', {}).get('mt_bench_jp', {}).get('overall_score', 0)
            if score > 0:
                scores.append((score, exp))
        
        if scores:
            scores.sort(key=lambda x: x[0])
            avg_score = sum([s[0] for s in scores]) / len(scores)
            
            report['summary']['average_mt_bench_score'] = avg_score
            report['summary']['best_experiment'] = scores[-1][1]['id']
            report['summary']['worst_experiment'] = scores[0][1]['id']
            
            # æ¨å¥¨äº‹é …
            if avg_score < 7.0:
                report['recommendations'].append("å¹³å‡ã‚¹ã‚³ã‚¢ãŒ7.0ã‚’ä¸‹å›ã£ã¦ã„ã¾ã™ã€‚ã‚ˆã‚Šå„ªç§€ãªãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã®ä½¿ç”¨ã‚’æ¤œè¨ã—ã¦ãã ã•ã„ã€‚")
            if len(completed_experiments) < 5:
                report['recommendations'].append("å®Ÿé¨“æ•°ãŒå°‘ãªã„ã§ã™ã€‚ã•ã¾ã–ã¾ãªçµ„ã¿åˆã‚ã›ã‚’è©¦ã—ã¦ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’è¦‹ã¤ã‘ã¦ãã ã•ã„ã€‚")
    
    # ãƒ¬ãƒãƒ¼ãƒˆã‚’JSONå½¢å¼ã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
    report_json = json.dumps(report, indent=2, ensure_ascii=False)
    
    st.download_button(
        label="ğŸ“‹ åŒ…æ‹¬çš„ãƒ¬ãƒãƒ¼ãƒˆã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
        data=report_json,
        file_name=f"comprehensive_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json",
        mime="application/json"
    )
    st.success("åŒ…æ‹¬çš„ãƒ¬ãƒãƒ¼ãƒˆãŒç”Ÿæˆã•ã‚Œã¾ã—ãŸï¼")


def cleanup_failed_experiments(app, experiments):
    """å¤±æ•—å®Ÿé¨“ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
    failed_experiments = [e for e in experiments if e.get('status') == 'failed']
    
    if not failed_experiments:
        st.info("å‰Šé™¤ã™ã‚‹å¤±æ•—å®Ÿé¨“ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
        return
    
    st.warning(f"{len(failed_experiments)}å€‹ã®å¤±æ•—å®Ÿé¨“ã‚’å‰Šé™¤ã—ã¾ã™ã‹ï¼Ÿ")
    
    if st.button("âš ï¸ å‰Šé™¤ã‚’å®Ÿè¡Œ", type="primary"):
        # æˆåŠŸå®Ÿé¨“ã®ã¿ã‚’ä¿æŒ
        successful_experiments = [e for e in experiments if e.get('status') != 'failed']
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’æ›´æ–°
        db_path = app.experiments_dir / 'experiments_db.json'
        with open(db_path, 'w') as f:
            json.dump(successful_experiments, f, indent=2)
        
        st.success(f"{len(failed_experiments)}å€‹ã®å¤±æ•—å®Ÿé¨“ã‚’å‰Šé™¤ã—ã¾ã—ãŸã€‚")
        st.rerun()


def optimize_experiment_data(app, experiments):
    """å®Ÿé¨“ãƒ‡ãƒ¼ã‚¿ã®æœ€é©åŒ–"""
    st.info("å®Ÿé¨“ãƒ‡ãƒ¼ã‚¿ã‚’æœ€é©åŒ–ä¸­...")
    
    optimized_experiments = []
    for exp in experiments:
        # ä¸è¦ãªãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’å‰Šé™¤
        optimized_exp = {
            'id': exp.get('id'),
            'merge_method': exp.get('merge_method'),
            'timestamp': exp.get('timestamp'),
            'status': exp.get('status'),
            'models': exp.get('models', []),
            'evaluations': exp.get('evaluations', {}),
            'quantization': exp.get('quantization', {}),
            'output_path': exp.get('output_path')
        }
        
        # ç©ºã®è©•ä¾¡çµæœã‚’å‰Šé™¤
        if not optimized_exp['evaluations']:
            del optimized_exp['evaluations']
        if not optimized_exp['quantization']:
            del optimized_exp['quantization']
            
        optimized_experiments.append(optimized_exp)
    
    # æœ€é©åŒ–ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜
    db_path = app.experiments_dir / 'experiments_db.json'
    with open(db_path, 'w') as f:
        json.dump(optimized_experiments, f, indent=2)
    
    st.success("å®Ÿé¨“ãƒ‡ãƒ¼ã‚¿ã‚’æœ€é©åŒ–ã—ã¾ã—ãŸã€‚")
    st.rerun()


if __name__ == "__main__":
    main()