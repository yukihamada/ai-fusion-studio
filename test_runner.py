#!/usr/bin/env python3
"""
è‡ªå‹•ãƒ†ã‚¹ãƒˆå®Ÿè¡Œã‚¹ã‚¯ãƒªãƒ—ãƒˆ
å…¨ã¦ã®ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œã—ã€çµæœã‚’ãƒ¬ãƒãƒ¼ãƒˆã¨ã—ã¦å‡ºåŠ›
"""

import subprocess
import sys
import json
import time
from pathlib import Path
from datetime import datetime


class TestRunner:
    """ãƒ†ã‚¹ãƒˆå®Ÿè¡Œç®¡ç†ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.project_root = Path(__file__).parent
        self.test_results = {
            'timestamp': datetime.now().isoformat(),
            'total_tests': 0,
            'passed': 0,
            'failed': 0,
            'skipped': 0,
            'duration': 0,
            'test_files': {}
        }
    
    def _parse_pytest_output(self, output):
        """pytestã®å‡ºåŠ›ã‹ã‚‰ãƒ†ã‚¹ãƒˆçµæœã‚’è§£æ"""
        import re
        
        # çµæœã‚µãƒãƒªãƒ¼ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ (ä¾‹: "3 failed, 19 passed, 1 deselected")
        summary_pattern = r'(\d+) failed, (\d+) passed|(\d+) passed'
        match = re.search(summary_pattern, output)
        
        if match:
            if match.group(1) and match.group(2):  # failed and passed
                self.test_results['failed'] = int(match.group(1))
                self.test_results['passed'] = int(match.group(2))
            elif match.group(3):  # only passed
                self.test_results['passed'] = int(match.group(3))
                self.test_results['failed'] = 0
            
            self.test_results['total_tests'] = self.test_results['passed'] + self.test_results['failed']
        
        # ã‚¹ã‚­ãƒƒãƒ—ã•ã‚ŒãŸãƒ†ã‚¹ãƒˆã‚’æ¢ã™
        skip_pattern = r'(\d+) skipped'
        skip_match = re.search(skip_pattern, output)
        if skip_match:
            self.test_results['skipped'] = int(skip_match.group(1))
    
    def run_unit_tests(self):
        """ãƒ¦ãƒ‹ãƒƒãƒˆãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œ"""
        print("ğŸ§ª ãƒ¦ãƒ‹ãƒƒãƒˆãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œä¸­...")
        
        start_time = time.time()
        
        # pytestã‚’å®Ÿè¡Œ
        cmd = [
            sys.executable, "-m", "pytest", 
            "tests/", 
            "-v", 
            "--tb=short",
            "--json-report",
            "--json-report-file=test_results.json",
            "-m", "not slow"  # é‡ã„ãƒ†ã‚¹ãƒˆã¯é™¤å¤–
        ]
        
        try:
            result = subprocess.run(cmd, capture_output=True, text=True, cwd=self.project_root)
            
            self.test_results['duration'] = time.time() - start_time
            self.test_results['exit_code'] = result.returncode
            self.test_results['stdout'] = result.stdout
            self.test_results['stderr'] = result.stderr
            
            # JSONãƒ¬ãƒãƒ¼ãƒˆãŒã‚ã‚Œã°èª­ã¿è¾¼ã¿
            json_report_path = self.project_root / "test_results.json"
            if json_report_path.exists():
                try:
                    with open(json_report_path, 'r') as f:
                        pytest_report = json.load(f)
                        summary = pytest_report.get('summary', {})
                        self.test_results['total_tests'] = summary.get('total', 0)
                        self.test_results['passed'] = summary.get('passed', 0)
                        self.test_results['failed'] = summary.get('failed', 0)
                        self.test_results['skipped'] = summary.get('skipped', 0)
                except (json.JSONDecodeError, KeyError) as e:
                    print(f"âš ï¸ JSONãƒ¬ãƒãƒ¼ãƒˆèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
                    # stdout/stderrã‹ã‚‰æƒ…å ±ã‚’æŠ½å‡º
                    self._parse_pytest_output(result.stdout)
            
            return result.returncode == 0
            
        except Exception as e:
            print(f"âŒ ãƒ†ã‚¹ãƒˆå®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def run_integration_tests(self):
        """çµ±åˆãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œï¼ˆè»½é‡ç‰ˆï¼‰"""
        print("ğŸ”— çµ±åˆãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œä¸­...")
        
        # åŸºæœ¬çš„ãªçµ±åˆãƒã‚§ãƒƒã‚¯
        integration_results = {
            'config_loading': self.test_config_loading(),
            'script_imports': self.test_script_imports(),
            'web_app_startup': self.test_web_app_startup()
        }
        
        self.test_results['integration'] = integration_results
        
        # ã™ã¹ã¦ã®çµ±åˆãƒ†ã‚¹ãƒˆãŒæˆåŠŸã—ãŸã‹ãƒã‚§ãƒƒã‚¯
        return all(integration_results.values())
    
    def test_config_loading(self):
        """è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ãƒ†ã‚¹ãƒˆ"""
        try:
            import yaml
            config_files = list((self.project_root / "configs").glob("*.yaml"))
            
            for config_file in config_files:
                with open(config_file, 'r') as f:
                    config = yaml.safe_load(f)
                    # å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’ãƒã‚§ãƒƒã‚¯
                    if 'merge_method' not in config or 'models' not in config:
                        return False
            
            return len(config_files) > 0
            
        except Exception as e:
            print(f"è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def test_script_imports(self):
        """ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆãƒ†ã‚¹ãƒˆ"""
        scripts_to_test = [
            'scripts.merge_models',
            'scripts.evaluate', 
            'scripts.quantize',
            'scripts.experiment_tracker'
        ]
        
        for script in scripts_to_test:
            try:
                __import__(script)
            except Exception as e:
                print(f"ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼ {script}: {e}")
                return False
        
        return True
    
    def test_web_app_startup(self):
        """Webã‚¢ãƒ—ãƒªã®èµ·å‹•ãƒ†ã‚¹ãƒˆ"""
        try:
            # Streamlitã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆãƒ†ã‚¹ãƒˆ
            import streamlit as st
            
            # ã‚¢ãƒ—ãƒªã‚¯ãƒ©ã‚¹ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆãƒ†ã‚¹ãƒˆ
            from web.app import LLMMergeLabApp
            
            # ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ä½œæˆãƒ†ã‚¹ãƒˆ
            app = LLMMergeLabApp()
            
            return True
            
        except Exception as e:
            print(f"Webã‚¢ãƒ—ãƒªèµ·å‹•ãƒ†ã‚¹ãƒˆå¤±æ•—: {e}")
            return False
    
    def run_performance_tests(self):
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆï¼ˆè»½é‡ç‰ˆï¼‰"""
        print("âš¡ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œä¸­...")
        
        performance_results = {}
        
        # ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®èµ·å‹•æ™‚é–“æ¸¬å®š
        scripts_to_test = [
            ('scripts/merge_models.py', ['--help']),
            ('scripts/evaluate.py', ['--help']),
            ('scripts/experiment_tracker.py', ['--help'])
        ]
        
        for script, args in scripts_to_test:
            start_time = time.time()
            try:
                result = subprocess.run(
                    [sys.executable, script] + args, 
                    capture_output=True, 
                    cwd=self.project_root,
                    timeout=10
                )
                end_time = time.time()
                
                performance_results[script] = {
                    'startup_time': end_time - start_time,
                    'success': result.returncode == 0
                }
                
            except subprocess.TimeoutExpired:
                performance_results[script] = {
                    'startup_time': 10.0,
                    'success': False,
                    'error': 'timeout'
                }
        
        self.test_results['performance'] = performance_results
        
        # ã™ã¹ã¦ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆãŒ5ç§’ä»¥å†…ã«èµ·å‹•ã™ã‚‹ã“ã¨ã‚’ç¢ºèª
        return all(
            result['startup_time'] < 5.0 and result['success'] 
            for result in performance_results.values()
        )
    
    def generate_report(self):
        """ãƒ†ã‚¹ãƒˆãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ"""
        print("\n" + "="*60)
        print("ğŸ“Š ãƒ†ã‚¹ãƒˆçµæœãƒ¬ãƒãƒ¼ãƒˆ")
        print("="*60)
        
        # åŸºæœ¬çµ±è¨ˆ
        total = self.test_results['total_tests']
        passed = self.test_results['passed']
        failed = self.test_results['failed']
        skipped = self.test_results['skipped']
        duration = self.test_results['duration']
        
        print(f"ğŸ§ª ãƒ¦ãƒ‹ãƒƒãƒˆãƒ†ã‚¹ãƒˆ:")
        print(f"   ç·æ•°: {total}")
        print(f"   âœ… æˆåŠŸ: {passed}")
        print(f"   âŒ å¤±æ•—: {failed}")
        print(f"   â­ï¸  ã‚¹ã‚­ãƒƒãƒ—: {skipped}")
        print(f"   â±ï¸  å®Ÿè¡Œæ™‚é–“: {duration:.2f}ç§’")
        
        if failed > 0:
            success_rate = (passed / total) * 100 if total > 0 else 0
            print(f"   ğŸ“ˆ æˆåŠŸç‡: {success_rate:.1f}%")
        
        # çµ±åˆãƒ†ã‚¹ãƒˆçµæœ
        if 'integration' in self.test_results:
            print(f"\nğŸ”— çµ±åˆãƒ†ã‚¹ãƒˆ:")
            for test_name, result in self.test_results['integration'].items():
                status = "âœ…" if result else "âŒ"
                print(f"   {status} {test_name}")
        
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆçµæœ
        if 'performance' in self.test_results:
            print(f"\nâš¡ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ:")
            for script, result in self.test_results['performance'].items():
                status = "âœ…" if result['success'] else "âŒ"
                time_str = f"{result['startup_time']:.2f}s"
                print(f"   {status} {script}: {time_str}")
        
        # ç·åˆåˆ¤å®š
        unit_success = failed == 0 and total > 0
        integration_success = all(self.test_results.get('integration', {}).values())
        performance_success = all(
            r['success'] for r in self.test_results.get('performance', {}).values()
        )
        
        overall_success = unit_success and integration_success and performance_success
        
        print(f"\n{'='*60}")
        if overall_success:
            print("ğŸ‰ ã™ã¹ã¦ã®ãƒ†ã‚¹ãƒˆãŒæˆåŠŸã—ã¾ã—ãŸï¼")
        else:
            print("âš ï¸  ä¸€éƒ¨ã®ãƒ†ã‚¹ãƒˆãŒå¤±æ•—ã—ã¾ã—ãŸã€‚")
        print(f"{'='*60}\n")
        
        # ãƒ¬ãƒãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜
        report_path = self.project_root / "test_report.json"
        with open(report_path, 'w') as f:
            json.dump(self.test_results, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ“„ è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆ: {report_path}")
        
        return overall_success
    
    def run_all_tests(self):
        """ã™ã¹ã¦ã®ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œ"""
        print("ğŸš€ LLM Merge Lab è‡ªå‹•ãƒ†ã‚¹ãƒˆã‚’é–‹å§‹ã—ã¾ã™...")
        print(f"ğŸ“… å®Ÿè¡Œæ—¥æ™‚: {self.test_results['timestamp']}")
        print()
        
        # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
        unit_success = self.run_unit_tests()
        integration_success = self.run_integration_tests()
        performance_success = self.run_performance_tests()
        
        # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        overall_success = self.generate_report()
        
        return overall_success


def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    runner = TestRunner()
    
    try:
        success = runner.run_all_tests()
        sys.exit(0 if success else 1)
    except KeyboardInterrupt:
        print("\nâ¹ï¸  ãƒ†ã‚¹ãƒˆãŒä¸­æ–­ã•ã‚Œã¾ã—ãŸ")
        sys.exit(1)
    except Exception as e:
        print(f"\nğŸ’¥ äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()